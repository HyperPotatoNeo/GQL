from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

from typing import List, Dict, Any, Optional, Tuple
import argparse, json, os, re, random
import numpy as np
import pandas as pd
from datasets import Dataset

# Unified scorer & task inference (your module)
from rewards.unified import get_task_name as infer_task, compute_score


# --------------------- helpers ---------------------
def _ensure_dir(path: str):
    os.makedirs(path, exist_ok=True)

def _save_parquet(rows: List[dict], output_path: str):
    df = pd.DataFrame(rows)
    df.to_parquet(output_path, index=False)

def _append_metrics_to_json(path: str, entry: dict):
    try:
        if os.path.exists(path):
            with open(path, "r") as f:
                data = json.load(f)
            if not isinstance(data, list):
                data = [data]
        else:
            data = []
    except Exception:
        data = []
    data.append(entry)
    with open(path, "w") as f:
        json.dump(data, f, indent=2)

def extract_question_from_prompt(prompt_cell: Any) -> str:
    """
    Supports a list of chat messages like:
      [{"role": "user", "content": "..."}]
    or a raw string. Returns the first user content when list[dict].
    """
    if isinstance(prompt_cell, list) and len(prompt_cell) > 0 and isinstance(prompt_cell[0], dict):
        return prompt_cell[0].get("content", "")
    return str(prompt_cell)


# --------------------- prefix conclusion detection ---------------------
_BOXED_RE = re.compile(r"\\boxed\s*\{.*?\}")

def prefix_has_concluded_math(prefix_text: str) -> bool:
    # For math-like tasks, treat any boxed answer present in the prefix as "concluded"
    return _BOXED_RE.search(prefix_text) is not None

def prefix_has_concluded_rg(prefix_text: str) -> bool:
    # Reasoning Gym style: <answer>...</answer> OR presence of "Final Answer:" already in the prefix
    if re.search(r"<answer>\s*.*?\s*</answer>", prefix_text, flags=re.DOTALL):
        return True
    return ("Final Answer:" in prefix_text)


# --------------------- prompt building ---------------------
def render_chat_template(tokenizer: AutoTokenizer, prompt: str) -> Tuple[str, List[Dict[str, str]]]:
    chat_message = [{"role": "user", "content": prompt}]
    rendered = tokenizer.apply_chat_template(chat_message, tokenize=False, add_generation_prompt=True)
    return rendered, chat_message

def render_with_assistant_prefix(
    tokenizer: AutoTokenizer, question: str, assistant_prefix: str
) -> Tuple[str, List[Dict[str, str]]]:
    """
    Return (rendered_prompt_string_for_generation, chat_messages_for_saving).
    We render the chat template with a single user message (generation prompt active)
    and then append the assistant prefix text to the rendered string so vLLM continues.
    For saving, we store a single-user message that asks the value model to score the prefix.
    """
    rendered_user, _ = render_chat_template(tokenizer, question)
    rendered_with_prefix = rendered_user + assistant_prefix

    value_prompt = (
        "You are given a question and a complete, partial, or empty response generated by yourself. "
        "Your role is to serve as a value function: Predict whether the response prefix is likely to lead to a correct final answer, "
        "or is the correct answer in case of a full response. If the response is empty, just try to assess how likely you are to solve the question. "
        "You do not need to complete the answer but you can if that is a good strategy. Instead, focus on evaluating the existing content and its potential "
        "to reach a correct conclusion.\n\n<question>"
    )
    full_prompt = (
        value_prompt + question + "</question>\n\n<response_prefix>: " + assistant_prefix +
        "</response_prefix>\n\nNow Reason step-by-step as a value function, then give a final score between 0.0 (guaranteed incorrect) to 1.0 (guaranteed correct) in \\boxed{}.\n\n"
    )
    chat_for_saving = [{"role": "user", "content": full_prompt}]
    return rendered_with_prefix, chat_for_saving


# --------------------- core pipeline ---------------------
def first_pass_generate(
    llm: LLM,
    tokenizer: AutoTokenizer,
    ds: Dataset,
    population: int,
    max_new_tokens: int,
    temperature: float,
) -> Tuple[
    List[str],               # questions
    List[List[str]],         # per_question_completions (population each)
    List[Any],               # ground truths (math uses string; rg unused)
    List[str],               # data_sources
    List[dict],              # extra_infos
]:
    """
    For each row: build the user-only prompt, generate `population` completions.
    Also collect per-row data_source and extra_info (for unified scorer).
    """
    questions: List[str] = []
    rendered_prompts: List[str] = []
    gts: List[Any] = []
    data_sources: List[str] = []
    extra_infos: List[dict] = []

    for row in ds:
        q = extract_question_from_prompt(row['prompt'])
        rendered, _ = render_chat_template(tokenizer, q)
        rendered_prompts.append(rendered)
        questions.append(q)

        # Robust getters
        data_sources.append(row.get('data_source', ''))
        extra_infos.append(row.get('extra_info', {}))

        # Ground truth: for math-like this is a string; for rg it's unused by compute_score
        if 'reward_model' in row and isinstance(row['reward_model'], dict) and 'ground_truth' in row['reward_model']:
            gts.append(row['reward_model']['ground_truth'])
        else:
            gts.append(None)

    sampling = SamplingParams(n=population, temperature=temperature, max_tokens=max_new_tokens)
    outs = llm.generate(rendered_prompts, sampling)

    per_question_completions: List[List[str]] = []
    for out in outs:
        texts = [o.text for o in out.outputs]  # length = population
        per_question_completions.append(texts)

    return questions, per_question_completions, gts, data_sources, extra_infos


def tokenize_lengths(tokenizer: AutoTokenizer, texts: List[str]) -> List[int]:
    """Return token lengths of each text (assistant responses)."""
    toks = tokenizer(texts, add_special_tokens=False)
    if isinstance(toks['input_ids'][0], list):
        return [len(ids) for ids in toks['input_ids']]
    else:
        return [len(toks['input_ids'])]


def sample_prefix_lengths_per_response(
    token_lengths: List[int],
    p_zero: float = 0.1,
    p_full: float = 0.1,
) -> List[int]:
    """
    For each response length L, sample a prefix length:
      - with p_zero -> 0
      - with p_full -> L
      - else -> uniform over 1..L-1 (when L >= 2); for L == 1 -> 1; for L == 0 -> 0
    This guarantees prefix_len âˆˆ [0, L], so no clamping against L is needed later.
    """
    out: List[int] = []
    for L in token_lengths:
        r = random.random()
        if L <= 0:
            out.append(0)
            continue
        if r < p_zero:
            out.append(0)
        elif r < p_zero + p_full:
            out.append(L)
        else:
            if L == 1:
                out.append(1)
            else:
                out.append(random.randint(1, L - 1))
    return out


def build_prefix_continuation_requests(
    tokenizer: AutoTokenizer,
    questions: List[str],
    data_sources: List[str],
    flat_first_pass: List[str],
    sampled_targets: List[int],
) -> Tuple[List[str], List[List[Dict[str, str]]], List[int], List[bool]]:
    """
    Build rendered prompts with an assistant prefix (truncated to sampled length).
    Returns:
      rendered_with_prefix (list of strings),
      chat_messages_for_saving (list of chat message lists),
      prefix_token_lengths (list of ints),
      concluded_flags (True if prefix already contains a final answer)
    """
    assert len(flat_first_pass) == len(sampled_targets)
    rendered_list: List[str] = []
    saved_chat_list: List[List[Dict[str, str]]] = []
    prefix_token_lens: List[int] = []
    concluded_flags: List[bool] = []

    # Tokenize all responses once to slice token-level prefixes
    enc = tokenizer(flat_first_pass, add_special_tokens=False)
    for idx, target_len in enumerate(sampled_targets):
        # target_len is guaranteed in [0, L]
        if target_len == 0:
            prefix_text = ""
        else:
            token_ids = enc["input_ids"][idx][:target_len]
            prefix_text = tokenizer.decode(token_ids, skip_special_tokens=True)

        q = questions[idx // population_global]
        rendered_with_prefix, chat_for_saving = render_with_assistant_prefix(tokenizer, q, prefix_text)
        rendered_list.append(rendered_with_prefix)
        saved_chat_list.append(chat_for_saving)
        prefix_token_lens.append(target_len)

        # Per-example task detection via data_source
        ds_src = data_sources[idx // population_global]
        task_i = infer_task(str(ds_src))
        concluded = prefix_has_concluded_rg(prefix_text) if task_i == 'rg' else prefix_has_concluded_math(prefix_text)
        concluded_flags.append(concluded)

    return rendered_list, saved_chat_list, prefix_token_lens, concluded_flags


def k_rollouts_from_prefix_masked(
    llm: LLM,
    rendered_with_prefix: List[str],
    need_indices: List[int],
    K: int,
    prefix_token_lens: List[int],
    temperature: float,
    total_assistant_cap: int = 8192,  # cap counts only assistant tokens: prefix + continuation
) -> Dict[int, List[str]]:
    """
    Generate only for indices in need_indices with per-query max_tokens so that:
      prefix_len_i + continuation_len_i <= total_assistant_cap
    We pass a list of SamplingParams to vLLM (one per-prompt).
    """
    if not need_indices:
        return {}

    prompts: List[str] = []
    params: List[SamplingParams] = []
    map_indices: List[int] = []
    zero_budget_indices: List[int] = []

    for i in need_indices:
        remain = max(0, total_assistant_cap - int(prefix_token_lens[i]))
        if remain == 0:
            zero_budget_indices.append(i)
            continue
        prompts.append(rendered_with_prefix[i])
        params.append(SamplingParams(n=K, temperature=temperature, max_tokens=remain))
        map_indices.append(i)

    result: Dict[int, List[str]] = {}

    if prompts:
        outs = llm.generate(prompts, params)  # per-query params supported by vLLM
        for i, out in zip(map_indices, outs):
            result[i] = [o.text for o in out.outputs]

    for i in zero_budget_indices:
        result[i] = ["" for _ in range(K)]

    return result


def compute_full_rewards_for_first_pass(
    data_sources: List[str],
    gts: List[Any],
    extra_infos: List[dict],
    flat_first_pass: List[str],
) -> List[float]:
    """
    Reward of the original full completions (used when prefix already concluded).
    """
    rewards: List[float] = []
    for idx, text in enumerate(flat_first_pass):
        q_idx = idx // population_global
        ds_src = data_sources[q_idx]
        gt = gts[q_idx]
        extra = extra_infos[q_idx]
        rewards.append(compute_score(ds_src, text, gt, extra))
    return rewards


def compute_mean_rewards_per_prefix(
    data_sources: List[str],
    gts: List[Any],
    extra_infos: List[dict],
    k_texts_dict: Dict[int, List[str]],
    n_total: int,
    full_rewards: List[float],
    concluded_flags: List[bool],
) -> List[float]:
    """
    For concluded prefixes: mean reward = full reward (no rollout).
    For others: mean over K continuations scored with unified compute_score.
    """
    mean_rewards = [0.0] * n_total
    for idx in range(n_total):
        if concluded_flags[idx]:
            mean_rewards[idx] = float(full_rewards[idx])
        else:
            q_idx = idx // population_global
            ds_src = data_sources[q_idx]
            gt = gts[q_idx]
            extra = extra_infos[q_idx]
            k_texts = k_texts_dict.get(idx, [])
            scores = [compute_score(ds_src, t, gt, extra) for t in k_texts]
            mean_rewards[idx] = float(np.mean(scores) if scores else 0.0)
    return mean_rewards


# --------------------- main pipeline ---------------------
def run_single_pass(
    model_name: str,
    dataset_path: str,
    output_dir: str,
    output_filename: str,
    k: int,
    population: int,
    max_new_tokens: int,
    temperature: float,
    tp_size: int,
    dtype: str,
    seed: int,
):
    global population_global
    population_global = population

    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
    llm = LLM(model=model_name, tensor_parallel_size=tp_size,
              dtype=dtype, trust_remote_code=True, seed=seed)
    ds = Dataset.from_parquet(dataset_path)
    #ds = ds.shuffle(seed=42).select(range(100))  # keep first 100 as you showed previously

    # 1) First pass: population trajectories per question
    questions, per_q_completions, gts, data_sources, extra_infos = first_pass_generate(
        llm=llm,
        tokenizer=tokenizer,
        ds=ds,
        population=population,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
    )

    # 2) Flatten all generated chains across dataset (size = num_items * population)
    flat_first_pass: List[str] = [c for comps in per_q_completions for c in comps]

    # 3) Token lengths (assistant responses only) in tokens
    flat_lengths: List[int] = tokenize_lengths(tokenizer, flat_first_pass)

    # 4) Sample prefix length for each response *individually*
    sampled_targets: List[int] = sample_prefix_lengths_per_response(flat_lengths, p_zero=0.1, p_full=0.1)

    # 5) Build continuation prompts with assistant prefixes & determine conclusion-in-prefix
    rendered_with_prefix, saved_chat_messages, prefix_token_lens, concluded_flags = build_prefix_continuation_requests(
        tokenizer=tokenizer,
        questions=questions,
        data_sources=data_sources,
        flat_first_pass=flat_first_pass,
        sampled_targets=sampled_targets,
    )

    # 6) Precompute rewards of full first-pass completions
    full_rewards = compute_full_rewards_for_first_pass(data_sources, gts, extra_infos, flat_first_pass)

    # 7) Identify which need rollouts (not concluded)
    need_indices = [i for i, c in enumerate(concluded_flags) if not c]

    # 8) Generate K continuations only for needed prefixes with per-query max tokens so that
    #    prefix_len + continuation_len <= max assistant cap (default 8192)
    k_texts_dict = k_rollouts_from_prefix_masked(
        llm=llm,
        rendered_with_prefix=rendered_with_prefix,
        need_indices=need_indices,
        K=k,
        prefix_token_lens=prefix_token_lens,
        temperature=temperature,
        total_assistant_cap=8192,
    )

    # 9) Assemble mean rewards per prefix
    mean_rewards: List[float] = compute_mean_rewards_per_prefix(
        data_sources=data_sources,
        gts=gts,
        extra_infos=extra_infos,
        k_texts_dict=k_texts_dict,
        n_total=len(rendered_with_prefix),
        full_rewards=full_rewards,
        concluded_flags=concluded_flags,
    )

    # 10) Save rows
    out_rows: List[dict] = []
    n_items = len(ds)
    for flat_idx, (chat_msgs, mr, plen) in enumerate(zip(saved_chat_messages, mean_rewards, prefix_token_lens)):
        q_idx = flat_idx // population
        row_meta = {
            "data_source": data_sources[q_idx],
            "original_index": q_idx,
            "population_member": flat_idx % population,
            "prefix_tokens": int(plen),
            "K": int(k),
            "population": int(population),
            "concluded_in_prefix": bool(concluded_flags[flat_idx]),
        }
        out_rows.append({
            "prompt": chat_msgs,  # [{"role":"user","content":...}]
            "reward_model": {"ground_truth": float(mr)},
            "extra_info": row_meta | (extra_infos[q_idx] if isinstance(extra_infos[q_idx], dict) else {}),
            "data_source": "value_estimation"
        })

    _ensure_dir(output_dir)
    out_parquet = os.path.join(output_dir, output_filename)
    _save_parquet(out_rows, out_parquet)

    metrics = {
        "n_original_items": n_items,
        "n_prefix_rows": len(out_rows),
        "k": k,
        "population": population,
        "mean_reward_over_all_prefixes": float(np.mean(mean_rewards)) if mean_rewards else 0.0,
        "std_reward_over_all_prefixes": float(np.std(mean_rewards)) if mean_rewards else 0.0,
        "rollout_fraction": float(len(need_indices)) / float(len(out_rows)) if out_rows else 0.0,
    }
    _append_metrics_to_json(os.path.join(output_dir, "prefix_value_summary.json"), metrics)
    print(json.dumps(metrics, indent=2))
    print(f"Wrote {len(out_rows)} rows to {out_parquet}")


# --------------------- CLI ---------------------
def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--model", default="Qwen/Qwen3-4B-Instruct-2507")
    ap.add_argument("--dataset", default="/pscratch/sd/s/siddart2/data/combined_proposals.parquet", help="Input parquet")
    ap.add_argument("--output", default="/pscratch/sd/s/siddart2/gql_data/qwen4b", help="Directory to write the new prefix-value parquet")
    ap.add_argument("--output_filename", default="aime.parquet", help="Output parquet filename")
    ap.add_argument("--k", type=int, default=8, help="K rollouts per prefix")
    ap.add_argument("--population", type=int, default=1, help="First-pass trajectories per question")
    ap.add_argument("--max-new-tokens", type=int, default=8192)
    ap.add_argument("--temperature", type=float, default=1.0)
    ap.add_argument("--tp-size", type=int, default=4)
    ap.add_argument("--dtype", default="bfloat16", choices=["auto","float16","bfloat16"])
    ap.add_argument("--seed", type=int, default=1234)
    args = ap.parse_args()

    random.seed(args.seed)
    np.random.seed(args.seed)

    run_single_pass(
        model_name=args.model,
        dataset_path=args.dataset,
        output_dir=args.output,
        output_filename=args.output_filename,
        k=args.k,
        population=args.population,
        max_new_tokens=args.max_new_tokens,
        temperature=args.temperature,
        tp_size=args.tp_size,
        dtype=args.dtype,
        seed=args.seed,
    )

if __name__ == "__main__":
    main()
